Будут практики каждую неделю.
Гвоздев же будет пытаться вести какую-нибудь теорию.
Будет экзамен.

Будет какой-то пулл вопросов к экзамену. Постарается в течение всех лекций дать ответ на все вопросы. Но если у вас есть какие-нибудь вопросы по ML, по работам, то можно спокойно к нему подходить. 

На этом предмете у нас ТИПА не нейросети.
А вот на "глубоком обучении"-таки есть нейросети.

В какие-то четверги Гвоздев будет появляться очно. В остальные дни будет дистант. 

Нужно написать на почту Гвоздеву, чтобы он узнал почту старосты. Чтобы было, кого предупреждать об изменениях в расписании.

Есть вариант в рамках практики "по сбору данных" и в рамках "машинного обучения" эти данные обработать. И это будет практическая работа сразу на два предмета. 

Кто не хочет учиться, но хочет зачет, может прислать ~~20~~ 50 задачек по программированию, которые требуют +- 20 строк для решения. Сложность должна быть +- вменяемой для вступительных экзаменов для магистратуры.

# Машинное обучение

## Задача классификации
### Метод ближайшего соседа
Обучение заключается в том, что мы создаем базу прецедентов, записываем её. Единственное, что нам нужно определить функцию, которая будет определять, насколько значения близки друг к другу. 

**Деревья решений** - тоже самое, что if-ы. Но if-ы определяет сама машина на основе набора наблюдений.
Есть куча наблюдений и результатов. Машина пытается найти такое разделение (if), чтобы она отделила два набора данных с более определенным ответом.

### Метод опорных векторов
Смысл в том, что вы ищете такую гиперплоскость, которая в наибольшей степени удалена от других точек.

Пока мы говорим только о задаче классификации. Наши построения позволяют нам быстро (не реализуя новые методы) получить "черный ящик", который позволяет создавать результаты классификации.

Модель не предсказывает правильно. Она предсказывает так, как бы ответил тот, кто различал выборку. 


### Задача регрессии
Если классификация пытается разделить объекты, то регрессия пытается предсказать значение функции.

Смысл в том, что мы можем взять некоторые множество данных и он будет апрксимировать функцию с некоторой точностью. При этом нужно понимать, что задача классификации (да/нет) и задача регрессии (функция оценки вероятности) +- одно и то же. 

### Задача понижения размерности 

Отложим по каждой оси какой-нибудь признак.  Только осей там не 2, а... 15. 
У нас есть много признаков, но при этом мы с очень высокой вероятностью можем сплюснуть кучу этих признаков лишь в парочку. 
Это получатся "скрытые признаки".
Понижение размености всё равно запихивает всё в какие-то оси. 

Помотрите web-сайт [scikit learn](https://scikit-learn.org/stable/index.html)

### Задача кластеризации
В задачах кластеризации мы не знаем, какие есть подмножества. Мы хотим понять, какие подмножества есть в данном наборе. 
Это задача поиска (разбиения) большого облака данных на множество маленьких.

**Пример**: в облаке произвольных точек берем одну и ищем ближайшие. Если расстояние между соседней точкой меньше какого-то расстояния, то берем её. И так пока не дойдем до рассматриваемого порога. 

Может быть применен для огромного количества задач. К примеру те же квартиры мы можем разбить на категории, имея лишь их описание. 

Мы можем взять некую модель, показать ей набор функций и сказать "сгенерируй набор".

### Дифузионные модели
Stable diffusion, midjourney и всё такое - вот это оно всё. 
Они генерируют шум и пытаются что-то выудить какие-то сигналы.

## Машинное обучение и пространственные данные
В абсолютном большинстве пространственные данные это чуть ли не худшее, с чем может работать машинное обучение.
Фундаментальное отличие пространственных данных от других - между координатами нельзя установить отношение порядка (одна координата не важнее другой).
А проблема в том, что методы машинного обучения всегда пытаются найти приоритет между признаками. Однако условные координаты x,y всегда равнозначны.

"Широта крымская, долгота колымская" :copyright: Гвоздев

Как работать с этим ужасом?
Ответ на поверхности - feature engineering. Вы создаете новые скалярные признаки и работаете с ними. А пространственные данные вы исключаете. 

Пример: те же квартиры. Можно взять такие признаки, как:
- расстояние метро до плоскости
- высота над уровнем земли, моря
- посчитать изохроны до парка, психбольницы =))
- ну или взять стоимость квартиры

Минус способа - нужно держать много данных.

Можно сделать lookup по базе данных и посмотреть, что происходит по рассматриваемым координатам.

В общем говоря один из признаков, которые можно извлечь из пространственных данных - расстояние от X до Y. A.k.a. "близость".

*Проблем в том, что принципиально это всё, что мы можем сделать.*

Применение методов машинного обучения на пространственных данных заключается в том, что мы хотим преобразовать эти данные к такой форме, с которыми удобнее всего было бы работать. 

Отдельно по ИИ, которые обрабатывают облака точек. Они умеют выделять сткруктурные признаки из пространственных паттернов, заключающиеся в соотношении двух точек. Но это опять же расстояние между точками. 

